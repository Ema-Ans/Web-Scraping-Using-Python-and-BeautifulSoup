{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3660e2-7c05-46d3-b26d-d7a2948a5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to request the data from the web servers\n",
    "import requests as r\n",
    "#works with a parser, to extract data from HTML \n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d68e7e1-5564-48fb-82b7-07f7332373a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the webpage content from the url\n",
    "webpag = r.get(\"https://attack.mitre.org/groups/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3a3287-bb12-4d51-b43e-64c709137002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to beautifulsoup\n",
    "soup = bs(webpag.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63bf270-b095-40e7-8705-60e01dff115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tableHead = soup.thead\n",
    "#tableHead  (if u want to get the tablehead)\n",
    "\n",
    "#if u want to extract the rowHeaders from the table head\n",
    "# rowHeaders = []\n",
    "# for x in tableHead.find_all('tr'):\n",
    "#     for y in x.find_all('th'):\n",
    "#         rowHeaders.append(y.text)\n",
    "# rowHeaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6544e91-2e83-47b9-bb79-36e303d295ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE JUNK -  FOR MY FUTURE REF\n",
    "# urlS = []\n",
    "# tableBody = soup.tbody\n",
    "#THIS IS HOW YOU CAN GET ALL THE LINKS FROM A Table.body\n",
    "# urls_list = tableBody.find_all('a')\n",
    "# print(urls_list)\n",
    "# for link in urls_list:\n",
    "#     href = link.get('href')\n",
    "#     urls.append(href)\n",
    "# urls2 = set(urls)\n",
    "# urls2List = list(urls2)\n",
    "# firstelem = urls2List[0]\n",
    "# newUrls = []\n",
    "# for elem in urls2List:\n",
    "#     if elem[:6] == '/group':\n",
    "#         newUrls.append(elem)\n",
    "#newUrls is the finalList of my Urls.\n",
    "#newUrls\n",
    "\n",
    "### CODE STARTS FROM HERE ###\n",
    "table1 = soup.find('table',{'class': \"table table-bordered table-alternate mt-2\"})\n",
    "\n",
    "#if you want to find the headers in the table\n",
    "# headers = []\n",
    "# for i in table1.find_all('th'):\n",
    "#     title = i.text\n",
    "#     headers.append(title)\n",
    "# headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86599f4e-e431-4a9b-9159-2d1cb8d097ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tableValues = []\n",
    "for x in table1.find_all('tr')[1:]:\n",
    "    #convert it to sets, to get rid of the duplicates\n",
    "    #because the same name is referenced multiple times \n",
    "    td_tags = list(set(x.find_all('a')))\n",
    "    td_val = [y for y in td_tags]\n",
    "    tableValues.append(td_val)\n",
    "#contains [[<a href=\"/software/S0012\">PoisonIvy</a, <a href=\"/groups/G0018\"> G0018 </a>,..] ...] for each row\n",
    "tableValues\n",
    "\n",
    "nameList = []\n",
    "#from that raw html list, we extract the names i.e., texts only for each group\n",
    "for elem in tableValues:\n",
    "    my_val = [y.text for y in elem]\n",
    "    nameList.append(list(set(my_val)))\n",
    "\n",
    "#for each of the elem in html list, we extract its link\n",
    "url_List = []\n",
    "for elem in tableValues:\n",
    "    my_urls = [x.get('href') for x in elem]\n",
    "    url_List.append(list(set(my_urls)))\n",
    "\n",
    "#only want names with ID's i.e., G****\n",
    "clean_nameList = []\n",
    "for x in nameList:\n",
    "    for y in x:\n",
    "        if y[1:3] ==  \"G0\" or y[1:3] == \"G1\":\n",
    "            clean_nameList.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3291047-9b88-4457-9e72-7c21247ff8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#only want the links for the ID's G****\n",
    "clean_urlList = []\n",
    "for i in range(len(url_List)):\n",
    "    elem = url_List[i]\n",
    "    name = clean_nameList[i][1:-1]\n",
    "    for x in elem:\n",
    "        if x[-5:] == name:\n",
    "            clean_urlList.append(x)\n",
    "            \n",
    "#now zip the two lists\n",
    "group_list = list(zip(clean_nameList, clean_urlList))\n",
    "# print(final_list[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a3ec626-8d15-4a56-b85a-e636eaf36e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now create the new_webpage thing using the urls collected from above:\n",
    "link = \"https://attack.mitre.org\" + group_list[0][1] + \"/\"\n",
    "#to load the webpage content from the url\n",
    "webpag1 = r.get(link)\n",
    "#convert to beautifulsoup\n",
    "soup = bs(webpag1.content,\"html.parser\")\n",
    "### CODE STARTS FROM HERE ###\n",
    "table_techniques = soup.find('table',{'class': \"table techniques-used background table-bordered\"})\n",
    "tableValues = []\n",
    "\n",
    "for x in table_techniques.find_all('tr')[1:]:\n",
    "    #we don't convert it to sets, to get rid of the duplicates\n",
    "    #because in this case, we care about the order for zipping in the end\n",
    "    td_tags = list(x.find_all('a'))\n",
    "    td_val = [y for y in td_tags]\n",
    "    tableValues.append(td_val)\n",
    "#contains [[<a href=\"/software/S0012\">PoisonIvy</a, <a href=\"/groups/G0018\"> G0018 </a>,..] ...] for each row\n",
    "tableValues\n",
    "\n",
    "nameList = []\n",
    "#from that raw html list, we extract the names i.e., texts only for each group\n",
    "for elem in tableValues:\n",
    "    my_val = [y.text for y in elem]\n",
    "    nameList.append(list(my_val))\n",
    "\n",
    "#for each of the elem in html list, we extract its link\n",
    "url_List = []\n",
    "for elem in tableValues:\n",
    "    my_urls = [x.get('href') for x in elem]\n",
    "    url_List.append(list(my_urls))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb33d21a-6bfd-47dd-bc84-ac94405d8650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('T1087', '/techniques/T1087'),\n",
       "  ('.001', '/techniques/T1087/001'),\n",
       "  ('Account Discovery', '/techniques/T1087'),\n",
       "  ('Local Account', '/techniques/T1087/001')],\n",
       " [('T1059', '/techniques/T1059'),\n",
       "  ('.003', '/techniques/T1059/003'),\n",
       "  ('Command and Scripting Interpreter', '/techniques/T1059'),\n",
       "  ('Windows Command Shell', '/techniques/T1059/003')],\n",
       " [('T1203', '/techniques/T1203'),\n",
       "  ('Exploitation for Client Execution', '/techniques/T1203')],\n",
       " [('T1083', '/techniques/T1083'),\n",
       "  ('File and Directory Discovery', '/techniques/T1083')],\n",
       " [('T1036', '/techniques/T1036'),\n",
       "  ('.005', '/techniques/T1036/005'),\n",
       "  ('Masquerading', '/techniques/T1036'),\n",
       "  ('Match Legitimate Name or Location', '/techniques/T1036/005')],\n",
       " [('T1069', '/techniques/T1069'),\n",
       "  ('.001', '/techniques/T1069/001'),\n",
       "  ('Permission Groups Discovery', '/techniques/T1069'),\n",
       "  ('Local Groups', '/techniques/T1069/001')],\n",
       " [('T1566', '/techniques/T1566'),\n",
       "  ('.001', '/techniques/T1566/001'),\n",
       "  ('Phishing', '/techniques/T1566'),\n",
       "  ('Spearphishing Attachment', '/techniques/T1566/001')],\n",
       " [('T1082', '/techniques/T1082'),\n",
       "  ('System Information Discovery', '/techniques/T1082')],\n",
       " [('T1016', '/techniques/T1016'),\n",
       "  ('System Network Configuration Discovery', '/techniques/T1016')],\n",
       " [('T1049', '/techniques/T1049'),\n",
       "  ('System Network Connections Discovery', '/techniques/T1049')],\n",
       " [('T1007', '/techniques/T1007'),\n",
       "  ('System Service Discovery', '/techniques/T1007')],\n",
       " [('T1204', '/techniques/T1204'),\n",
       "  ('.002', '/techniques/T1204/002'),\n",
       "  ('User Execution', '/techniques/T1204'),\n",
       "  ('Malicious File', '/techniques/T1204/002')]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip these two:\n",
    "final = list(zip(nameList, url_List))\n",
    "#print(final[0])\n",
    "techniques_List = []\n",
    "for elem in final:\n",
    "    techniques = elem[0]\n",
    "    links = elem[1]\n",
    "    temp  = []\n",
    "    for i in range(len(links)):\n",
    "        if links[i][:11] == \"/techniques\":\n",
    "            temp.append((techniques[i], links[i]))\n",
    "    techniques_List.append(temp)\n",
    "            \n",
    "techniques_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e3dd1db-d367-445a-8f05-0526e962b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do the similar thing to extract all the software used and it's url\n",
    "table_software = soup.find('table',{'class': \"table table-bordered table-alternate mt-2\"})\n",
    "tableValues = []\n",
    "\n",
    "for x in table_software.find_all('tr')[1:]:\n",
    "    #we dont convert it to sets, because we care about the order\n",
    "    td_tags = list(x.find_all('a'))\n",
    "    td_val = [y for y in td_tags]\n",
    "    tableValues.append(td_val)\n",
    "#contains [[<a href=\"/software/S0012\">PoisonIvy</a, <a href=\"/groups/G0018\"> G0018 </a>,..] ...] for each row\n",
    "tableValues\n",
    "\n",
    "nameList = []\n",
    "#from that raw html list, we extract the names i.e., texts only for each group\n",
    "for elem in tableValues:\n",
    "    my_val = [y.text for y in elem]\n",
    "    nameList.append(list(my_val))\n",
    "\n",
    "#for each of the elem in html list, we extract its link\n",
    "url_List = []\n",
    "for elem in tableValues:\n",
    "    my_urls = [x.get('href') for x in elem]\n",
    "    url_List.append(list(my_urls))\n",
    "#zip these two:\n",
    "final = list(zip(nameList, url_List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25edc3b5-92fb-4c52-b9e9-01d7ccd587dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('S0043', '/software/S0043'), ('BUBBLEWRAP', '/software/S0043')],\n",
       " [('S0100', '/software/S0100'), ('ipconfig', '/software/S0100')],\n",
       " [('S0042', '/software/S0042'), ('LOWBALL', '/software/S0042')],\n",
       " [('S0039', '/software/S0039'), ('Net', '/software/S0039')],\n",
       " [('S0104', '/software/S0104'), ('netstat', '/software/S0104')],\n",
       " [('S0012', '/software/S0012'), ('PoisonIvy', '/software/S0012')],\n",
       " [('S0096', '/software/S0096'), ('Systeminfo', '/software/S0096')]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "software_List = []\n",
    "for elem in final:\n",
    "    techniques = elem[0]\n",
    "    links = elem[1]\n",
    "    temp  = []\n",
    "    for i in range(len(links)):\n",
    "        if links[i][:9] == \"/software\":\n",
    "            temp.append((techniques[i], links[i]))\n",
    "    software_List.append(temp)\n",
    "            \n",
    "software_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98efd98-6a75-4e27-a01d-cc7fc9228a94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
